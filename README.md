# Project 3 â€“ Adversarial Attacks on Image Classifiers

This project explores adversarial attacks like FGSM and PGD on deep learning models. The goal is to understand how small input perturbations can mislead image classifiers and reduce their reliability.

## Key Features
- Implemented FGSM and PGD attacks
- Visualized attack effects on model predictions
- Evaluated accuracy drop under adversarial settings

## Dataset
- CIFAR-10

## Requirements
- Python
- PyTorch
- Matplotlib

## How to Run
```bash
pip install torch torchvision matplotlib
jupyter notebook Adversarial_Attacks.ipynb
